from ai_media_generator.utils import download_file, ensure_directory_exists, clean_text_for_filename
from dotenv import load_dotenv
import os
from google.cloud import texttospeech
from moviepy.editor import ImageSequenceClip, AudioFileClip
from pydub import AudioSegment
import openai
import requests

# Load environment variables
load_dotenv()

# Retrieve API keys and credentials
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found. Ensure it's set in the .env file.")
if not GOOGLE_APPLICATION_CREDENTIALS:
    raise ValueError("GOOGLE_APPLICATION_CREDENTIALS not found. Ensure it's set in the .env file.")

# Set Google Application Credentials
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = GOOGLE_APPLICATION_CREDENTIALS

# Configure OpenAI API
openai.api_key = OPENAI_API_KEY

def generate_text(prompt):
    """Generates text using ChatGPT."""
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
    )
    return response['choices'][0]['message']['content']

def generate_audio_google_tts(text, output_path="assets/audio/output.mp3"):
    """Generates audio from text using Google Cloud Text-to-Speech."""
    client = texttospeech.TextToSpeechClient()

    # Set the text input
    synthesis_input = texttospeech.SynthesisInput(text=text)

    # Select the voice and audio configuration
    voice = texttospeech.VoiceSelectionParams(
        language_code="en-US",
        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
    )
    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

    # Perform the text-to-speech request
    response = client.synthesize_speech(
        input=synthesis_input, voice=voice, audio_config=audio_config
    )

    # Save the audio file
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "wb") as out:
        out.write(response.audio_content)
    return output_path

def generate_images_for_scenes(prompt, generated_text, max_length=1000):
    """
    Splits the generated text into scenes and generates an image for each scene.

    Args:
        prompt (str): The original user prompt.
        generated_text (str): The text generated by the AI model.
        max_length (int): Maximum length of each scene's text (default is 1000).

    Returns:
        list: A list of file paths for the generated images.
    """
    import textwrap

    # Add extra context to the user prompt for better scene splitting
    extended_prompt = f"Using the following text as a story: '{prompt}', divide the story into scenes."
    scenes = textwrap.wrap(generated_text, max_length)

    image_paths = []
    for i, scene in enumerate(scenes):
        # Add scene-specific context for generating an image
        scene_prompt = f"Scene {i + 1}: {scene.strip()}"

        try:
            # Call the OpenAI API to generate an image for the scene
            response = openai.Image.create(
                prompt=scene_prompt,
                n=1,  # Generate one image per scene
                size="1024x1024"
            )
            # Save the generated image
            image_url = response['data'][0]['url']
            output_path = f"assets/images/scene_{i + 1}.png"
            download_file(image_url, output_path)  # Use the utility function from utils.py
            image_paths.append(output_path)
        except openai.error.OpenAIError as e:
            print(f"Error generating image for Scene {i + 1}: {e}")

    return image_paths


from pydub import AudioSegment

def create_video(images, audio_path, output_path="assets/video/output.mp4"):
    """
    Combines one image per scene with its corresponding audio into a video.

    Args:
        images (list): List of image file paths (one per scene).
        audio_path (str): Path to the audio file for the narration.
        output_path (str): Path to save the generated video.

    Returns:
        str: The path to the generated video.
    """
    # Load the audio file
    audio = AudioSegment.from_file(audio_path)

    # Calculate the duration of each scene in milliseconds
    scene_duration = len(audio) // len(images)

    # Create an image sequence clip with each image lasting for the scene duration
    durations = [scene_duration / 1000] * len(images)  # Convert milliseconds to seconds
    clip = ImageSequenceClip(images, durations=durations)

    # Add the audio to the video
    audio_clip = AudioFileClip(audio_path)
    clip = clip.set_audio(audio_clip)

    # Write the video to the output file
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    clip.write_videofile(output_path, codec="libx264", audio_codec="aac")

    return output_path



if __name__ == "__main__":
    user_prompt = input("Enter a prompt: ")

    # Modify the prompt to instruct GPT to generate text divided into scenes
    modified_prompt = (
        f"Using the following input: '{user_prompt}', write a story and divide it into clear scenes. "
        "Each scene should have a brief description, separated by the phrase 'Scene [number]:'."
    )

    # Generate text based on the modified prompt
    generated_text = generate_text(modified_prompt)
    print(f"Generated Text:\n{generated_text}")

    # Extract scenes from the generated text
    print("Generating images for each scene...")
    image_paths = []
    scenes = [
        line.strip()
        for line in generated_text.split('\n')
        if line.strip().startswith("Scene") or line.strip().startswith("**Scene")
    ]

    for idx, scene in enumerate(scenes, start=1):
        try:
            response = openai.Image.create(
                prompt=scene,
                n=1,  # Generate one image per scene
                size="1024x1024"
            )
            image_url = response['data'][0]['url']
            output_path = f"assets/images/scene_{idx}.png"
            download_file(image_url, output_path)  # Use the utility function
            image_paths.append(output_path)
        except openai.error.OpenAIError as e:
            print(f"Error generating image for {scene}: {e}")

    print(f"Generated {len(image_paths)} images for the scenes.")

    # Generate audio for the full story
    print("Generating audio for the story...")
    audio_path = generate_audio_google_tts(generated_text)

    # Create a video combining the images and audio
    print("Creating video...")
    video_path = create_video(image_paths, audio_path)
    print(f"Video created at {video_path}")
